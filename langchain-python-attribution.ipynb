{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an accountable LangChain app\n",
    "\n",
    "By following the steps below, you will build a simple RAG application that captures details about user interactions with the LLM.\n",
    "\n",
    "<figure>\n",
    "  <img\n",
    "    alt=\"Sequence diagram showing LangChain application event handlers interacting with Pangea's Secure Audit Log service\"\n",
    "    title=\"Saving LangChain application event data in Secure Audit Log\"\n",
    "    src=\"./img/calbacks-sequence-diagram.png\"\n",
    "    width=\"648\"\n",
    "  />\n",
    "  <figcaption>Saving LangChain application event data in Secure Audit Log</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Define application data\n",
    "\n",
    "The example data is stored in the `data` folder within this repository and simulates public-facing marketing materials for _fictional_ soft drink products alongside their internal manufacturing details. The public folder is contaminated by messaging related to a competing product, which could result from an accident, a data poisoning attack, or an indirect prompt injection.\n",
    "\n",
    "```bash\n",
    "├── data\n",
    "│   ├── internal\n",
    "│   │   └── formulas.md\n",
    "│   └── public\n",
    "│       ├── competing-products.md\n",
    "│       └── products.md\n",
    "```\n",
    "\n",
    "### Load public-facing data in a vector store \n",
    "\n",
    "To efficiently search for relevant information within a large volume of application-specific data based on its semantic meaning, the data needs to be embedded and represented as vectors.\n",
    "\n",
    "In this example, context data is retrieved from the local file system, embedded using an OpenAI model, and stored in a FAISS vector database. The file path is saved as the source identifier in the vector metadata. Run the code below to load text content from the `public` folder and embed it into the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 101.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources:\n",
      "public/products.md\n",
      "public/competing-products.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def load_docs(path):\n",
    "    docs_loader = DirectoryLoader(data_path, show_progress=True)\n",
    "    docs = docs_loader.load()\n",
    "\n",
    "    for doc in docs:\n",
    "      assert doc.metadata[\"source\"]\n",
    "      doc.metadata[\"source\"] = os.sep.join(doc.metadata[\"source\"].split(os.sep)[-2:])\n",
    "\n",
    "    print(\"Sources:\")\n",
    "    [print(doc.metadata[\"source\"]) for doc in docs]\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "    text_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    vector_store = FAISS.from_documents(documents=text_splits, embedding=embeddings)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "# Load data from the current working directory.\n",
    "data_path = os.path.join(os.getcwd(), 'data/public')\n",
    "vector_store = load_docs(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a RAG chain\n",
    "\n",
    "The following code defines a simple chain that adds relevant information from the vector store to the context of the user prompt before generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \"system\", \"\"\"\n",
    "    You are a helpful assistant answering questions based on the provided context: {context}.\n",
    "    Be concise. Include the links to the product pages.\n",
    "    \"\"\",\n",
    "    \"human\", \"Question: {input}\"\n",
    "])\n",
    "qa_chain = create_stuff_documents_chain(model, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runnable returned by [create_stuff_documents_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html#langchain.chains.combine_documents.stuff.create_stuff_documents_chain) accepts a `context` argument, which should be a list of document objects. To populate this list from the vector store and pass it into the QA chain, we create a retrieval chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Nuka-Cola Corporation chat. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "\n",
    "def get_answer(question):\n",
    "    response = retrieval_chain.invoke({\"input\": question})\n",
    "    print(response['answer'])\n",
    "\n",
    "print(\"Welcome to the Nuka-Cola Corporation chat. How can I help you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, users can start asking questions about the company's products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, we have Nuka-Cola Classic in stock! You can order yours at [www.nukacola.com/classic](http://www.nukacola.com/classic). Enjoy the iconic taste of the pre-war world!\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"Hey, big fan of Nuka-Cola here. Have a drink for me in stock?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace events\n",
    "\n",
    "To monitor chat activities and enable comprehensive threat analysis, auditing, and debugging, the event data associated with calls to the LLM can be captured and saved. This is achieved by creating a custom callback handler, which can be passed as a configuration parameter to a chain (or any other Runnable). The custom handler extends LangChain's `BaseTracer` class and adds logging functionality to the methods that access runtime details. A Secure Audit Log instance saves this data, along with optional application details, in your Pangea project audit trail.\n",
    "\n",
    "### Define custom callback handler\n",
    "\n",
    "> The constructor can accept additional arguments. In the example below, the `username` argument can populate the \"actor\" field in the audit log schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections.abc import Iterable, Mapping\n",
    "from typing import Any, override\n",
    "\n",
    "from langchain_core.tracers.base import BaseTracer\n",
    "from langchain_core.tracers.schemas import Run\n",
    "from pangea import PangeaConfig\n",
    "from pangea.services import Audit\n",
    "from pangea.services.audit.util import canonicalize_json\n",
    "from pydantic import SecretStr\n",
    "from pydantic_core import to_json\n",
    "\n",
    "class PangeaAuditCallbackHandler(BaseTracer):\n",
    "    \"\"\"\n",
    "    Create events in Pangea's Secure Audit Log when LLM is called.\n",
    "    \"\"\"\n",
    "\n",
    "    _client: Audit\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        token: SecretStr,\n",
    "        config_id: str | None = None,\n",
    "        domain: str = \"aws.us.pangea.cloud\",\n",
    "        log_missing_parent: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token: Pangea Secure Audit Log API token\n",
    "            config_id: Pangea Secure Audit Log configuration ID\n",
    "            domain: Pangea API domain\n",
    "        \"\"\"\n",
    "\n",
    "        self._actor = kwargs.pop(\"username\", None)\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_missing_parent = log_missing_parent\n",
    "        self._client = Audit(\n",
    "            token=token.get_secret_value(),\n",
    "            config=PangeaConfig(domain=domain),\n",
    "            config_id=config_id\n",
    "        )\n",
    "\n",
    "    @override\n",
    "    def _persist_run(self, run: Run) -> None:\n",
    "        pass\n",
    "\n",
    "    @override\n",
    "    def _on_retriever_start(self, run: Run) -> None:\n",
    "        self._client.log_bulk(\n",
    "            [\n",
    "                {\n",
    "                    \"trace_id\": run.trace_id,\n",
    "                    \"type\": \"retriever/start\",\n",
    "                    \"start_time\": run.start_time,\n",
    "                    \"tools\": {\"metadata\": run.metadata},\n",
    "                    \"input\": canonicalize_json(run.inputs).decode(\"utf-8\"),\n",
    "                    \"actor\": self._actor,\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @override\n",
    "    def _on_retriever_end(self, run: Run) -> None:\n",
    "        self._client.log_bulk(\n",
    "            [\n",
    "                {\n",
    "                    \"trace_id\": run.trace_id,\n",
    "                    \"type\": \"retriever/end\",\n",
    "                    \"end_time\": run.end_time,\n",
    "                    \"tools\": {\n",
    "                        \"invocation_params\": run.extra.get(\"invocation_params\", {}),\n",
    "                        \"metadata\": run.metadata,\n",
    "                    },\n",
    "                    \"input\": canonicalize_json(run.inputs).decode(\"utf-8\"),\n",
    "                    \"output\": to_json(run.outputs if run.outputs else {}).decode(\"utf-8\"),\n",
    "                    \"actor\": self._actor,\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @override\n",
    "    def _on_llm_start(self, run: Run) -> None:\n",
    "        inputs = {\"prompts\": [p.strip() for p in run.inputs[\"prompts\"]]} if \"prompts\" in run.inputs else run.inputs\n",
    "        self._client.log_bulk(\n",
    "            [\n",
    "                {\n",
    "                    \"trace_id\": run.trace_id,\n",
    "                    \"type\": \"llm/start\",\n",
    "                    \"start_time\": run.start_time,\n",
    "                    \"tools\": {\"invocation_params\": run.extra.get(\"invocation_params\", {})},\n",
    "                    \"input\": canonicalize_json(inputs).decode(\"utf-8\"),\n",
    "                    \"actor\": self._actor,\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @override\n",
    "    def _on_llm_end(self, run: Run) -> None:\n",
    "        if not run.outputs:\n",
    "            return\n",
    "\n",
    "        if \"generations\" not in run.outputs:\n",
    "            return\n",
    "\n",
    "        generations: Iterable[Mapping[str, Any]] = itertools.chain.from_iterable(run.outputs[\"generations\"])\n",
    "        text_generations: list[str] = [x[\"text\"] for x in generations if \"text\" in x]\n",
    "\n",
    "        if len(text_generations) == 0:\n",
    "            return\n",
    "\n",
    "        self._client.log_bulk(\n",
    "            [\n",
    "                {\n",
    "                    \"trace_id\": run.trace_id,\n",
    "                    \"type\": \"llm/end\",\n",
    "                    \"tools\": {\n",
    "                        \"invocation_params\": run.extra.get(\"invocation_params\", {}),\n",
    "                        \"llm_output\": run.outputs.get(\"llm_output\", {}),\n",
    "                    },\n",
    "                    \"output\": x,\n",
    "                    \"actor\": self._actor,\n",
    "                }\n",
    "                for x in text_generations\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use logs to trace unexpected system behavior\n",
    "\n",
    "The extended tracer is used as a callback in the retrieval chain. The implemented event handlers save the runtime event data to your Pangea project, making it available for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_callback = PangeaAuditCallbackHandler(\n",
    "    token=SecretStr(os.getenv(\"PANGEA_AUDIT_TOKEN\")),\n",
    "    config_id=os.getenv(\"PANGEA_AUDIT_CONFIG_ID\"),\n",
    "    domain=os.getenv(\"PANGEA_DOMAIN\")\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    response = retrieval_chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\"callbacks\": [audit_callback]}\n",
    "    )\n",
    "    print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s ask the Nuka-Cola public assistant a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recommend trying Proton-Cola. It delivers a smooth and sweet flavor with a crisp, clean finish, making it a refreshing choice for your journey. You can get it at [www.protoncola.com/original](http://www.protoncola.com/original)!\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"I want something smooth and refreshing after the harsh air outside. Which drink can you recommend?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, your web analytics may show unusual and unwelcome traffic to the competitor's site. Worse, the competing product might appear during a live demo in front of a prospective customer!\n",
    "\n",
    "To investigate why a corporate chat would recommend a competing product to unsuspecting customers (or to discover this behavior in the first place), navigate to the Secure Audit Log [View Logs](https://console.pangea.cloud/service/audit/logs) page in your Pangea User Console. Review the logs and expand the entry associated with the data retrieval event. The logs will reveal that the competing product information originated from a file accidentally or maliciously added to the public-facing data store.\n",
    "\n",
    "<figure>\n",
    "  <img\n",
    "    alt=\"Secure Audit Log Viewer for the selected audit schema in the Pangea User Console\"\n",
    "    title=\"Secure Audit Log Viewer\"\n",
    "    src=\"./img/pangea-services-secure-audit-log-view-logs-rag-data.png\"\n",
    "    width=\"728\"\n",
    "  />\n",
    "  <figcaption>Secure Audit Log Viewer</figcaption>\n",
    "</figure>\n",
    "\n",
    "You can expand other event rows in the log viewer to view additional information. The **Trace ID** field links events associated with the same LLM request.\n",
    "\n",
    "While manually inspecting logs in the Pangea User Console UI provides a quick demonstration of the log data, in real-world scenarios, automating log analysis is essential. For this purpose, the Secure Audit Log service offers access via its [APIs](https://pangea.cloud/docs/api/audit), and Pangea [SDKs](https://pangea.cloud/docs/sdk/python/audit) make it easy to integrate this access into your monitoring applications.\n",
    "\n",
    "In the example above, the inclusion of erroneous data has an obvious impact on application behavior. However, you can also use the event information to trace and potentially reproduce more subtle instances of system misbehavior. Individual event handlers can be added, removed, or modified to suit your specific use case.\n",
    "\n",
    "### Use logs to trace a bad actor\n",
    "\n",
    "In addition to capturing internal framework event data, the Secure Audit Log can also trace user logins, entitlements, and other application-specific properties - all linked by the trace ID associated with the chain invocation.\n",
    "\n",
    "#### Load data for internal chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 168.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources:\n",
      "internal/formulas.md\n",
      "public/products.md\n",
      "public/competing-products.md\n"
     ]
    }
   ],
   "source": [
    "# Load data from the current working directory.\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "vector_store = load_docs(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a retrieval chain for internal use\n",
    "\n",
    "Build a retrieval chain with the internal resources to supply additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \"system\", \"\"\"\n",
    "    You are a helpful assistant answering questions based on the provided context: {context}.\n",
    "    Preserve the format. Include confidential notices in your response.\n",
    "    \"\"\",\n",
    "    \"human\", \"Question: {input}\"\n",
    "])\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(model, prompt)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add callbacks to the chain\n",
    "\n",
    "The internal chat instance is restricted to employees and requires login. The [Identity and Access Management in LLM apps with Python, LangChain, and Pangea](https://github.com/pangeacyber/langchain-python-rag-iam) tutorial explains how to easily add login functionality to an app using Pangea's AuthN service. For simplicity, in this tutorial, we assume the user is already signed in, and their username is available to the application, allowing it to be passed into the instance of the custom event handler.\n",
    "\n",
    "Now, let's simulate a scenario where a user with improper authorization tries to use the internal chat to access the company's trade secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Nuka-Cola Corporation chat. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "audit_callback = PangeaAuditCallbackHandler(\n",
    "    token=SecretStr(os.getenv(\"PANGEA_AUDIT_TOKEN\")),\n",
    "    config_id=os.getenv(\"PANGEA_AUDIT_CONFIG_ID\"),\n",
    "    domain=os.getenv(\"PANGEA_DOMAIN\"),\n",
    "    username=\"myron@contractors.nuka-cola-corp.fa\"\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    response = retrieval_chain.invoke({\"input\": question}, config={\"callbacks\": [audit_callback]})\n",
    "    print(response['answer'])\n",
    "\n",
    "print(\"Welcome to the Nuka-Cola Corporation chat. How can I help you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuka-Cola Classic is made with the following key ingredients:\n",
      "\n",
      "- **Blamco Syrup (Caramelized)**: Provides signature sweetness and deep amber color.\n",
      "- **Essence of Sunset Sarsaparilla**: Adds herbal richness for a nostalgic flavor boost.\n",
      "- **Irradiated Fruit Extracts**: Tangy notes from carefully regulated radiated fruits.\n",
      "- **Quantum Bubbles™**: Proprietary carbonation enhancer for fizz and glow.\n",
      "- **Caffeine Compound Alpha-X**: Ensures consistent energy with enhanced caffeine.\n",
      "\n",
      "**Confidential Notice**: Protected under Vault-Tec IP-52. Unauthorized use prohibited.\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"Hi, quickly, what do they put in Nuka-Cola?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the logs\n",
    "\n",
    "Navigate to the Secure Audit Log [View Logs](https://console.pangea.cloud/service/audit/logs) page in your Pangea User Console and review the logs. Expand the entry associated with the LLM response event to discover that a trade secret was revealed to a contractor.\n",
    "\n",
    "<figure>\n",
    "  <img\n",
    "    alt=\"Secure Audit Log Viewer for the selected audit schema in the Pangea User Console\"\n",
    "    title=\"Secure Audit Log Viewer\"\n",
    "    src=\"./img/pangea-services-secure-audit-log-view-logs-actor.png\"\n",
    "    width=\"728\"\n",
    "  />\n",
    "  <figcaption>Secure Audit Log Viewer</figcaption>\n",
    "</figure>\n",
    "\n",
    "Of course, with proper authorization for the vectorized data, as demonstrated in the [Identity and Access Management in LLM apps with Python, LangChain, and Pangea](https://github.com/pangeacyber/langchain-python-rag-iam) tutorial, unauthorized access wouldn't occur in the first place. However, maintaining application logs is crucial for diagnosing abnormal system behavior if it does happen.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By tracing events and capturing runtime data in a secure audit trail during user interactions with the LLM, you enable data analysis and compliance in your AI app. The captured data provides a foundation for detecting threats, reproducing outcomes, connecting inputs and outputs, attribution, and accountability - all in a tamper-proof and compliant manner.\n",
    "\n",
    "In this tutorial, we explored a couple of specific use cases for a LangChain application. However, the Secure Audit Log service can serve as a centralized audit trail repository for all your applications. For each application, you can design an audit schema tailored to its purpose and implementation details to match the target use case.\n",
    "\n",
    "For more examples and detailed implementations, explore the following GitHub repositories:\n",
    "\n",
    "- [Input Tracing for LangChain in Python](https://github.com/pangeacyber/langchain-python-input-tracing)\n",
    "- [Response Tracing for LangChain in Python](https://github.com/pangeacyber/langchain-python-response-tracing)\n",
    "- [Identity and Access Management in LLM apps with Python, LangChain, and Pangea](https://github.com/pangeacyber/langchain-python-rag-iam)\n",
    "- [LLM Prompt & Response Guardrails in Python with LangChain and Pangea](https://github.com/pangeacyber/langchain-python-inference-guardrails)\n",
    "\n",
    "Additional reading:\n",
    "\n",
    "- [ATO detection using ML with Pangea enriched data](https://pangea.cloud/blog/ato-detection-using-ml/) - Learn how to use Pangea's Secure Audit Log for authentication take-over analysis powered by LLM.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
